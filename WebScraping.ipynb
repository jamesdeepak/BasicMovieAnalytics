{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Importing python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "%matplotlib inline\n",
    "import sys \n",
    "import re\n",
    "from random import randint\n",
    "import random \n",
    "from time import sleep\n",
    "\n",
    "import dateutil.parser\n",
    "\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use http://www.boxofficemojo.com/robots.txt to see access to which sites are blocked and for other info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to parse data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_date(datestring):\n",
    "    try:\n",
    "        date = dateutil.parser.parse(datestring)\n",
    "        return date\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "\n",
    "def money_to_int(moneystring):\n",
    "    try:\n",
    "        moneystring = moneystring.replace('$', '').replace(',', '')\n",
    "        return int(moneystring)\n",
    "    except:\n",
    "        try:\n",
    "            if 'million' in moneystring or 'mil' in moneystring:\n",
    "                moneystring = moneystring.replace('$', '').replace(',', '').replace('million','').replace('mil','').replace('mill','')\n",
    "                moneyInMil = int(moneystring)*1000000\n",
    "                return moneyInMil\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "def runtime_to_minutes(runtimestring):\n",
    "    runtime = runtimestring.split()\n",
    "    try:\n",
    "        minutes = int(runtime[0])*60 + int(runtime[2])\n",
    "        return minutes\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Go to main starting page from where, click on each year and get information about movies for that particular year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if needed: pip install requests\n",
    "import requests\n",
    "\n",
    "mainURL = 'http://www.boxofficemojo.com/yearly/'\n",
    "\n",
    "headers = {\n",
    "    'user-Agent': 'Mozilla/5.0'\n",
    "    \n",
    "}\n",
    "\n",
    "response = requests.get(mainURL, headers = headers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deepakgautam/anaconda/lib/python3.5/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "page = response.text\n",
    "soup = BeautifulSoup(page)\n",
    "#print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "#### Get basic info for each year along with links to enther individual year page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "page_1_TrList = soup.find_all('table')[1].find_all('table')[0].find_all('tr')\n",
    "\n",
    "overallTable_df = pd.DataFrame(columns=('Year', 'Gross', 'GrossChange','TicketSold','TSChange','TotalMovies','TotalScreens','AvgTicketPrice','AvgCost','TopMovie'))\n",
    "page1_LinkList =[]\n",
    "#skipping first row as it contains header items\n",
    "for rowCount, tr in enumerate(page_1_TrList[1:]):\n",
    "    tdList = tr.find_all('td')\n",
    "    colVals = []\n",
    "    \n",
    "    for tdIndex, td in enumerate(tdList):\n",
    "\n",
    "        #getting redirect link from the first item\n",
    "        if tdIndex ==0:\n",
    "            smLink = td.find_all('font')[0].find('a')['href']\n",
    "            fullLink = mainURL + smLink\n",
    "        \n",
    "        item = td.find_all('font')[0].text\n",
    "        \n",
    "        colVals.append(item)   \n",
    "    \n",
    "    page1_LinkList.append(fullLink)\n",
    "    overallTable_df.loc[rowCount] = colVals\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write Overall Yearly Table to CSV file\n",
    "overallTable_df.to_csv('/Users/deepakgautam/BasicMovieAnalytics/Files/yearSummary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "errorPageList = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function which takes a link for yearly page and find individual movies info for that year and also links for those movies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getDataFromByYearPages(inputURL ,releaseYear):\n",
    "    #movie row starts from index 2 and goes up to index 101\n",
    "    \n",
    "    headers = {\n",
    "        'user-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "    \n",
    "    #print(inputURL)\n",
    "    response = requests.get(inputURL, headers = headers)\n",
    "    \n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    \n",
    "    columns=('Rank', 'MovieName', 'Studio','TotalDomestic','TheaterCountTotal','TotalOpeningWeekend','TheaterCountOW','ReleaseDate')\n",
    "    \n",
    "    temp_allMovieURLList = []\n",
    "    \n",
    "\n",
    "    #movie row starts from index 2 and goes up to index \n",
    "    \n",
    "    try:\n",
    "        totalRows = len(soup.find_all('table')[4].find_all('table')[1].find_all('tr'))\n",
    "    except:\n",
    "        totalRows = 0\n",
    "        errorPageList.append(inputURL)\n",
    "    \n",
    "    data = []\n",
    "    for i in range(2,totalRows-4):\n",
    "        \n",
    "        curTDList = soup.find_all('table')[4].find_all('table')[1].find_all('tr')[i].find_all('td')\n",
    "        colVals = []\n",
    "        for tdIndex, td in enumerate(curTDList):\n",
    "            \n",
    "            if tdIndex > 7:\n",
    "                break # some pages have closing dates and some page do not .. so we will stop after \n",
    "            \n",
    "            #getting redirect link from the first item\n",
    "            if tdIndex ==1:\n",
    "                \n",
    "                try:\n",
    "                    smLink = td.find_all('font')[0].find('a')['href']\n",
    "                    movieURL = 'http://www.boxofficemojo.com' + str(smLink)\n",
    "\n",
    "                    if movieURL not in allMovieURLList and movieURL not in temp_allMovieURLList:\n",
    "                        temp_allMovieURLList.append(movieURL)\n",
    "                except:\n",
    "                     e = sys.exc_info()[0]\n",
    "                    \n",
    "            \n",
    "            item = td.find_all('font')[0].text\n",
    "            \n",
    "            if tdIndex in range(3,7):\n",
    "                item = money_to_int(item)\n",
    "            \n",
    "            if tdIndex ==7:\n",
    "                item = item + '/' + str(releaseYear) # since opening week date does not have year info\n",
    "                item = to_date(item)\n",
    "                \n",
    "            colVals.append(item)\n",
    "        \n",
    "        #print(i)\n",
    "        data.append(colVals)\n",
    "        \n",
    "    return(pd.DataFrame(data, columns=columns), temp_allMovieURLList)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loop through each link list(yearly) and get lists of movies for that year\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.boxofficemojo.com/yearly/chart/?yr=2016&p=.htm\n",
      "http://www.boxofficemojo.com/yearly/chart/?yr=2015&p=.htm\n",
      "http://www.boxofficemojo.com/yearly/chart/?yr=2014&p=.htm\n",
      "http://www.boxofficemojo.com/yearly/chart/?yr=2013&p=.htm\n",
      "http://www.boxofficemojo.com/yearly/chart/?yr=2012&p=.htm\n",
      "http://www.boxofficemojo.com/yearly/chart/?yr=2011&p=.htm\n",
      "http://www.boxofficemojo.com/yearly/chart/?yr=2010&p=.htm\n",
      "http://www.boxofficemojo.com/yearly/chart/?yr=2009&p=.htm\n",
      "http://www.boxofficemojo.com/yearly/chart/?yr=2008&p=.htm\n",
      "http://www.boxofficemojo.com/yearly/chart/?yr=2007&p=.htm\n",
      "http://www.boxofficemojo.com/yearly/chart/?yr=2006&p=.htm\n",
      "http://www.boxofficemojo.com/yearly/chart/?yr=2005&p=.htm\n",
      "http://www.boxofficemojo.com/yearly/chart/?yr=2004&p=.htm\n",
      "http://www.boxofficemojo.com/yearly/chart/?yr=2003&p=.htm\n",
      "http://www.boxofficemojo.com/yearly/chart/?yr=2002&p=.htm\n",
      "http://www.boxofficemojo.com/yearly/chart/?yr=2001&p=.htm\n",
      "http://www.boxofficemojo.com/yearly/chart/?yr=2000&p=.htm\n",
      "http://www.boxofficemojo.com/yearly/chart/?yr=1999&p=.htm\n",
      "http://www.boxofficemojo.com/yearly/chart/?yr=1998&p=.htm\n",
      "http://www.boxofficemojo.com/yearly/chart/?yr=1997&p=.htm\n",
      "http://www.boxofficemojo.com/yearly/chart/?yr=1996&p=.htm\n",
      "http://www.boxofficemojo.com/yearly/chart/?yr=1995&p=.htm\n",
      "http://www.boxofficemojo.com/yearly/chart/?yr=1994&p=.htm\n",
      "http://www.boxofficemojo.com/yearly/chart/?yr=1993&p=.htm\n",
      "http://www.boxofficemojo.com/yearly/chart/?yr=1992&p=.htm\n",
      "http://www.boxofficemojo.com/yearly/chart/?yr=1991&p=.htm\n",
      "http://www.boxofficemojo.com/yearly/chart/?yr=1990&p=.htm\n",
      "http://www.boxofficemojo.com/yearly/chart/?yr=1989&p=.htm\n",
      "http://www.boxofficemojo.com/yearly/chart/?yr=1988&p=.htm\n",
      "http://www.boxofficemojo.com/yearly/chart/?yr=1987&p=.htm\n",
      "http://www.boxofficemojo.com/yearly/chart/?yr=1986&p=.htm\n",
      "http://www.boxofficemojo.com/yearly/chart/?yr=1985&p=.htm\n",
      "http://www.boxofficemojo.com/yearly/chart/?yr=1984&p=.htm\n",
      "http://www.boxofficemojo.com/yearly/chart/?yr=1983&p=.htm\n",
      "http://www.boxofficemojo.com/yearly/chart/?yr=1982&p=.htm\n",
      "http://www.boxofficemojo.com/yearly/chart/?yr=1981&p=.htm\n",
      "http://www.boxofficemojo.com/yearly/chart/?yr=1980&p=.htm\n"
     ]
    }
   ],
   "source": [
    "# instantiate a list which keeps track of URL for every movie for every year that we are looping through\n",
    "\n",
    "allMovieURLList = []\n",
    "main_movie_df = pd.DataFrame(columns=('Rank', 'MovieName', 'Studio','TotalDomestic','TheaterCountTotal','TotalOpeningWeekend','TheaterCountOW','ReleaseDate'))\n",
    "\n",
    "successFulLinks_page1 = []\n",
    "data_dfs = []\n",
    "temp_links = []\n",
    "\n",
    "for pageLink in page1_LinkList: # remove :1 when you have dealt response of one page\n",
    "    time.sleep(random.uniform(1,3))\n",
    "    mainURL = pageLink\n",
    "\n",
    "    headers = {\n",
    "        'user-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "    response = requests.get(mainURL, headers = headers)\n",
    "    \n",
    "    #print(pageLink)\n",
    "    if response.status_code == 200:\n",
    "        successFulLinks_page1.append(True)\n",
    "        \n",
    "        page = response.text\n",
    "        soup = BeautifulSoup(page, \"lxml\")\n",
    "        \n",
    "        \n",
    "        # soup.find_all('table')[3] has information about different pages in each year.\n",
    "        rawPageList = soup.find_all('table')[3].find_all('tr')[0].find('td').find('b').find_all('a')\n",
    "        pageList_inPage2 = []\n",
    "        \n",
    "        for curList in rawPageList:\n",
    "            pageList_inPage2.append(str('http://www.boxofficemojo.com/' + curList['href']))\n",
    "            \n",
    "        \n",
    "        \n",
    "        # call a function that gets page2 _ current page_ movie link and gives back list of movies and info in a dataset\n",
    "        # Remember that you will update allMovieURLList from this function\n",
    "        # Append this to main Movie Table\n",
    "        \n",
    "        temp_allMovieURLList = []\n",
    "        \n",
    "        curYear = mainURL[-11:-7]\n",
    "        tempMovie_df, temp_allMovieURLList = getDataFromByYearPages(mainURL, curYear)\n",
    "        \n",
    "        data_dfs.append(tempMovie_df)        \n",
    "        temp_links = temp_links + temp_allMovieURLList\n",
    "        \n",
    "        # Then loop thorugh all items pageList_inPage2 and call the same function as above and save their info in Movie table\n",
    "        temp_allMovieURLList = []\n",
    "        for curPage in pageList_inPage2:\n",
    "            time.sleep(1)\n",
    "            tempMovie_df, temp_allMovieURLList = getDataFromByYearPages(curPage,curYear)\n",
    "            \n",
    "            data_dfs.append(tempMovie_df)\n",
    "            temp_links = temp_links + temp_allMovieURLList\n",
    "    else:\n",
    "        successFulLinks_page1.append(False)\n",
    "\n",
    "main_movie_df = pd.concat(data_dfs)\n",
    "\n",
    "allMovieURLList = temp_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14095"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## write mainmovie table and all movie url into csv file \n",
    "#main_movie_df.to_csv('/Users/deepakgautam/sf16_ds3/projects/02-luther/local/allMovies.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to get additional info for individual movie from the individual movie link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getMovieValues(soup):\n",
    "    valList = []\n",
    "    \n",
    "    columns=('Movie', 'Distributor','Genre','RunTime','MPAA','Budget','ForeignSale')\n",
    "    \n",
    "    try:\n",
    "        curMovieName = soup.find_all('font')[1].text\n",
    "    except:\n",
    "        curMovieName = None\n",
    "\n",
    "    try:\n",
    "        #soup.find_all('table')[5].find_all('tr')[0].find('b').text\n",
    "        curDist = soup.find_all('table')[5].find_all('tr')[1].find_all('b')[0].text\n",
    "    except:\n",
    "        curDist= None\n",
    "\n",
    "    try:\n",
    "        curGenre = soup.find_all('table')[5].find_all('tr')[2].find_all('b')[0].text\n",
    "    except:\n",
    "        curGenre= None\n",
    "\n",
    "    try:\n",
    "        raw_curRuntime = soup.find_all('table')[5].find_all('tr')[2].find_all('b')[1].text\n",
    "        curRuntime = runtime_to_minutes(raw_curRuntime)\n",
    "    except:\n",
    "        curRuntime= None\n",
    "\n",
    "    try:\n",
    "        curMPAA = soup.find_all('table')[5].find_all('tr')[3].find_all('b')[0].text\n",
    "    except:\n",
    "        curMPAA = None\n",
    "\n",
    "    try:\n",
    "        raw_curBudget = soup.find_all('table')[5].find_all('tr')[3].find_all('b')[1].text\n",
    "        curBudget = money_to_int(raw_curBudget)\n",
    "    except:\n",
    "        curBudget= None\n",
    "\n",
    "    try:\n",
    "        raw_ForeignSale = soup.find_all('table')[9].find_all('tr')[1].find_all('td')[1].text\n",
    "        ForeignSale = money_to_int(raw_ForeignSale)\n",
    "    except:\n",
    "        ForeignSale = None\n",
    "       \n",
    "    valList.append(curMovieName)\n",
    "    valList.append(curDist)\n",
    "    valList.append(curGenre)\n",
    "    valList.append(curRuntime)\n",
    "    valList.append(curMPAA)\n",
    "    valList.append(curBudget)\n",
    "    valList.append(ForeignSale)\n",
    "    \n",
    "    \n",
    "    return(pd.DataFrame([valList], columns=columns))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through values of allMovieURLList to get individual movie's additional info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500\n",
      "13000\n",
      "13500\n",
      "14000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Ind_movie_df = pd.DataFrame(columns=('Movie', 'Distributor','Genre','RunTime','MPAA','Budget','ForeignSale'))\n",
    "\n",
    "successFulLinks_IndPages = []\n",
    "data_dfs = []\n",
    "count = 0\n",
    "errorPageList_indMovies = []\n",
    "\n",
    "for pageLink in allMovieURLList: # remove :1 when you have dealt response of one page\n",
    "    time.sleep(random.uniform(0.5,1.5))\n",
    "    mainURL = pageLink\n",
    "\n",
    "    headers = {\n",
    "        'user-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "    response = requests.get(mainURL, headers = headers)\n",
    "    \n",
    "    #print(pageLink)\n",
    "    if response.status_code == 200:\n",
    "        successFulLinks_IndPages.append(True)\n",
    "        \n",
    "        page = response.text\n",
    "        soup = BeautifulSoup(page, \"lxml\")\n",
    "        \n",
    "        try:\n",
    "            tempMovie_df = getMovieValues(soup)\n",
    "        except:\n",
    "            errorPageList_indMovies.append(pageLink)\n",
    "        \n",
    "        data_dfs.append(tempMovie_df)\n",
    "        \n",
    "    else:\n",
    "        successFulLinks_IndPages.append(False)\n",
    "        \n",
    "    # in every 500 movies write to csv file, so that if you lose connection you at least have a file\n",
    "    \n",
    "    \n",
    "    if count > 1 and count%500 == 0:\n",
    "        Ind_movie_df = pd.concat(data_dfs)\n",
    "        Ind_movie_df.to_csv('/Users/deepakgautam/BasicMovieAnalytics/Files/indMovieInfo.csv')\n",
    "        #print(count)\n",
    "        \n",
    "    if count > 1  and count%10 == 0:\n",
    "        with open(\"/Users/deepakgautam/BasicMovieAnalytics/Files/movieProgress.txt\", \"a\") as myfile:\n",
    "            myfile.write(\"Scrapping complete for \" + str(count + 1) + \" movies. \\n\")\n",
    "    \n",
    "    count += 1 \n",
    "    \n",
    "        \n",
    "Ind_movie_df = pd.concat(data_dfs)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## To save Individual Movie info as csv file\n",
    "#Ind_movie_df.to_csv('/Users/deepakgautam/BasicMovieAnalytics/Files/indMovieInfo.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### To save all movie url List\n",
    "## pd.DataFrame({\"movie_name\": allMovieURLList}).to_csv(\"/Users/deepakgautam/BasicMovieAnalytics/Files/allMovieURL_List.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
